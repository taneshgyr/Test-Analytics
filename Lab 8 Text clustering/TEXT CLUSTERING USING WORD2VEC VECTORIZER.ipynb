{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b55fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from gensim.models import Word2Vec\n",
    "from tabulate import tabulate\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e9b4ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "240a29fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [\n",
    "    \"I love playing football on the weekends\",\n",
    "    \"I enjoy hiking and camping in the mountains\",\n",
    "    \"I like to read books and watch movies\",\n",
    "    \"I prefer playing video games over sports\",\n",
    "    \"I love listening to music and going to concerts\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4a619aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "# Step 2: Convert to lowercase\n",
    "def convert_to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Step 3: Tokenization\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Step 4: Remove stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Step 5: Stemming (Optional)\n",
    "def perform_stemming(tokens):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fcd5a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dataset = []\n",
    "for document in dataset:\n",
    "    document = remove_punctuation(document)\n",
    "    document = convert_to_lowercase(document)\n",
    "    tokens = tokenize_text(document)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    # You can choose to perform stemming here if needed\n",
    "    # tokens = perform_stemming(tokens)\n",
    "    preprocessed_dataset.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ce2ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: ['love', 'playing', 'football', 'weekends']\n",
      "Document 2: ['enjoy', 'hiking', 'camping', 'mountains']\n",
      "Document 3: ['like', 'read', 'books', 'watch', 'movies']\n",
      "Document 4: ['prefer', 'playing', 'video', 'games', 'sports']\n",
      "Document 5: ['love', 'listening', 'music', 'going', 'concerts']\n"
     ]
    }
   ],
   "source": [
    "for i, document in enumerate(preprocessed_dataset, 1):\n",
    "    print(f\"Document {i}: {document}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a147fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = [doc.split() for doc in dataset]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_dataset, vector_size=100,\n",
    "window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e34257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([np.mean([word2vec_model.wv[word] for word in doc.split() if word in\n",
    "word2vec_model.wv], axis=0) for doc in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22c9aa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "D:\\Anaconda\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            0\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              1\n",
      "I prefer playing video games over sports                           0\n",
      "I love listening to music and going to concerts                    1\n"
     ]
    }
   ],
   "source": [
    "k = 2 # Define the number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "# Tabulate the document and predicted cluster\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "738fcf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703bb3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
