{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23320dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\ACER\n",
      "[nltk_data]     NITRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\ACER\n",
      "[nltk_data]     NITRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\ACER\n",
      "[nltk_data]     NITRO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Download NLTK Resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3467393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "data = pd.read_csv('news_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cafdf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use only the 'text' column and drop rows with null values\n",
    "documents = data['text'].dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324c7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stopwords, lemmatizer, and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalnum() and not token.isdigit()] # Remove non-alphanumeric tokens and numbers\n",
    "    tokens = [token for token in tokens if token not in stop_words] # Remove stopwords\n",
    "    tokens = [stemmer.stem(token) for token in tokens] # Apply stemming\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] # Apply lemmatization\n",
    "    return tokens\n",
    "\n",
    "# Preprocess each document in the list\n",
    "preprocessed_documents = [preprocess_text(doc) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92e41150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gensim Dictionary object from the preprocessed documents\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "\n",
    "# Convert each preprocessed document into a bag-of-words representation using the dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56392e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an LDA model on the corpus with 4 topics using Gensim's LdaModel class\n",
    "lda_model = LdaModel(corpus, num_topics=4, id2word=dictionary, passes=15)\n",
    "\n",
    "# Calculate the coherence score for the LDA model\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b63259e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store dominant topic labels for each document\n",
    "article_labels = []\n",
    "\n",
    "# Iterate over each processed document\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    "    # Convert to bag-of-words representation\n",
    "    bow = dictionary.doc2bow(doc)\n",
    "    # Get list of topic probabilities\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    # Determine topic with highest probability\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    # Append to the list\n",
    "    article_labels.append(dominant_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a966deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with the Articles and Topic:\n",
      "                                             The Article  The Topic\n",
      "0      I was wondering if anyone out there could enli...          1\n",
      "1      I recently posted an article asking what kind ...          1\n",
      "2      \\nIt depends on your priorities.  A lot of peo...          1\n",
      "3      an excellent automatic can be found in the sub...          1\n",
      "4      : Ford and his automobile.  I need information...          1\n",
      "...                                                  ...        ...\n",
      "11091  Secrecy in Clipper Chip\\n\\nThe serial number o...          3\n",
      "11092  Hi !\\n\\nI am interested in the source of FEAL ...          3\n",
      "11093  The actual algorithm is classified, however, t...          2\n",
      "11094  \\n\\tThis appears to be generic calling upon th...          2\n",
      "11095  \\nProbably keep quiet and take it, lest they g...          2\n",
      "\n",
      "[11096 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame({\"The Article\": documents, \"The Topic\": article_labels})\n",
    "\n",
    "# Print the DataFrame\n",
    "print(\"Table with the Articles and Topic:\")\n",
    "print(df)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e65e2aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "-\"q\" (weight: 0.042)\n",
      "-\"max\" (weight: 0.040)\n",
      "-\"g\" (weight: 0.026)\n",
      "-\"r\" (weight: 0.025)\n",
      "-\"p\" (weight: 0.021)\n",
      "-\"db\" (weight: 0.019)\n",
      "-\"n\" (weight: 0.017)\n",
      "-\"k\" (weight: 0.014)\n",
      "-\"w\" (weight: 0.014)\n",
      "-\"c\" (weight: 0.012)\n",
      "\n",
      "Topic 1:\n",
      "-\"use\" (weight: 0.010)\n",
      "-\"get\" (weight: 0.008)\n",
      "-\"one\" (weight: 0.008)\n",
      "-\"would\" (weight: 0.007)\n",
      "-\"like\" (weight: 0.007)\n",
      "-\"know\" (weight: 0.005)\n",
      "-\"window\" (weight: 0.005)\n",
      "-\"game\" (weight: 0.005)\n",
      "-\"run\" (weight: 0.004)\n",
      "-\"work\" (weight: 0.004)\n",
      "\n",
      "Topic 2:\n",
      "-\"would\" (weight: 0.009)\n",
      "-\"peopl\" (weight: 0.008)\n",
      "-\"one\" (weight: 0.008)\n",
      "-\"say\" (weight: 0.005)\n",
      "-\"think\" (weight: 0.005)\n",
      "-\"know\" (weight: 0.005)\n",
      "-\"like\" (weight: 0.004)\n",
      "-\"go\" (weight: 0.004)\n",
      "-\"time\" (weight: 0.004)\n",
      "-\"make\" (weight: 0.004)\n",
      "\n",
      "Topic 3:\n",
      "-\"x\" (weight: 0.018)\n",
      "-\"key\" (weight: 0.012)\n",
      "-\"use\" (weight: 0.011)\n",
      "-\"encrypt\" (weight: 0.008)\n",
      "-\"file\" (weight: 0.007)\n",
      "-\"program\" (weight: 0.006)\n",
      "-\"system\" (weight: 0.006)\n",
      "-\"inform\" (weight: 0.006)\n",
      "-\"secur\" (weight: 0.005)\n",
      "-\"chip\" (weight: 0.005)\n",
      "\n",
      "Topic Coherence Score (C_V): 0.6634\n"
     ]
    }
   ],
   "source": [
    "# Print the top terms for each topic\n",
    "print(\"Top Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"-{word.strip()} (weight: {weight.strip()})\")\n",
    "    print()\n",
    "\n",
    "# Display the coherence score\n",
    "print(f'Topic Coherence Score (C_V): {coherence_lda:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c51ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
